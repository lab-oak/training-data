This is a small offering of ~33 million positions to train a RBY battle network. See 'include/train/compressed-frames.h' in github.com/lab-oak/oak for the details of the data format. The format is universal since we store the actual battle bytes rather than any particular encoding of the battle.

This is frankly too little data to adequately train a network like on implemented in above repo. Nevertheless, this dataset (actually a superset with ~50 million positions - too large for Github) was able to train a network that outperforms the agent that produced the dataset at the with the same number of search iterations. This suggests that incrementally stronger nets can be trained by using the previous network iteration to generate new data.

The data was generated by self-play using monte-carlo eval at leaf nodes. Moves were selected by computing the nash equilibrium of the root matrix and sampling from that. Because of the repo size limitation, the team generation training data (the .build files) is omitted. The build network used was freshly initialized, so its policy inference is basically uniformly random. The teams used are the Smogon RBY OU samples, with information sporadically deleted and then filled in by the network. The spefics are printed in the 'args' file in this directory, but basically 75% of teams were unchanged from their base sample team.
