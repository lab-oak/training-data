This is a small offering of ~50 million positions (.battle files) to train a RBY battle network. See 'include/train/compressed-frames.h' in github.com/lab-oak/oak for the details of the data format. The format is universal since we store the actual battle bytes rather than any particular encoding of the battle.

This is frankly too little data to adequately train a network like the one implemented in Oak. Nevertheless, this dataset was able to train a network that outperforms the agent that produced the dataset using the same number of search iterations. This suggests that incrementally stronger nets can be trained by using the previous network iteration to generate new data.

The data was generated by self-play using monte-carlo eval at leaf nodes. Moves were selected by computing the Nash equilibrium of the root matrix and sampling from that. The team-builder network used was freshly initialized, so its policy inference is basically uniformly random. The teams used are the Smogon RBY OU samples, with information sporadically deleted and then filled in by the network. The spefics are printed in the 'args' file in this directory, but basically 75% of teams were unchanged from their base sample team.

The team-builder data (.build files) is still untested. It contains only trajectories and two different reward targets. Therefore, we include the build network parameters since many policy learning algorithms are "on-policy". See 'include/train/build-trajectory.h' for details.
